{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ba7105",
   "metadata": {},
   "source": [
    "# Understanding the Impact of Image Quality and Distance of Objects to Object Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bd6c9",
   "metadata": {},
   "source": [
    "***A reproduction*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c63682",
   "metadata": {},
   "source": [
    "## Downscale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d24d59",
   "metadata": {},
   "source": [
    "Folder expansion: Some datasets come with multiple subfolders not needed for the purposes of our reproduction study. Example:\n",
    "\n",
    "```\n",
    "img\n",
    "|_dir1 \n",
    "    |_img1.png\n",
    "    |_img2.png\n",
    "    |_img3.png\n",
    "|_dir2\n",
    "    |_img1.png\n",
    "    |_img2.png\n",
    "    |_img3.png\n",
    "|_dir3\n",
    "(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_folders import expand_folders\n",
    "expand_folders(\"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e06b67",
   "metadata": {},
   "source": [
    "### Spatial and Amplitudinal Resolution Downsampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302f025",
   "metadata": {},
   "source": [
    "Spatial - 1.42x Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058db19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/spatially_compressed\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    scale_factors=[720.0/1024], #value from paper \n",
    "    qp_values=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326eaffb",
   "metadata": {},
   "source": [
    "Amplitudinal Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/eurocity_original_amplitudinally_compressed\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "\n",
    "qp_values = [16, 24, 34, 38, 46] #values from paper\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    scale_factors=[],\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    qp_values=qp_values, #values from paper\n",
    "    expansion = \"amplitudinal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c61eb",
   "metadata": {},
   "source": [
    "Mixed Downsampling - Combined Set of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f356ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/mixed\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "qp_values = [16, 24, 34, 38, 46] #values from paper\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    scale_factors=[1, 720.0/1024, 854.0/ 1920], #values from paper\n",
    "    qp_values=[0, 5, 10, 16, 20, 24, 28, 34, 38, 42, 46, 51], #values from paper\n",
    "    expansion=\"mixed\", \n",
    "    subsample_spatial=True,\n",
    "    subsample_amplitudinal=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57854c",
   "metadata": {},
   "source": [
    "## Train/Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2259e3",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca7ee050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=ra_yolo5l.yaml, data=/home/vasil/Desktop/frmdl/datasets/mixed/data.yaml, hyp=/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=1, batch_size=1, imgsz=800, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=cpu, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/runs/train, name=exp, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, ra_yolo=True, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 1 commit. Use 'git pull ultralytics master' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ 2025-6-10 Python-3.13.2 torch-2.6.0+cu124 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1  18878464  models.common.Conv                      [1024, 2048, 3, 2]            \n",
      " 10                -1  3  39866368  models.common.C3                        [2048, 2048, 3]               \n",
      " 11                -1  1  10491904  models.common.SPPF                      [2048, 2048, 5]               \n",
      " 12                -1  1   2099200  models.common.Conv                      [2048, 1024, 1, 1]            \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3  11020288  models.common.C3                        [2048, 1024, 3, False]        \n",
      " 16                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 25                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 26           [-1, 1]  1         0  models.common.Concat                    [1]                           \n",
      " 27                -1  3    173312  models.common.C3                        [256, 128, 3, False]          \n",
      " 28                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 29          [-1, 24]  1         0  models.common.Concat                    [1]                           \n",
      " 30                -1  3    625152  models.common.C3                        [256, 256, 3, False]          \n",
      " 31                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 32          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 33                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 34                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 35          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 36                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 37                -1  1   9439232  models.common.Conv                      [1024, 1024, 3, 2]            \n",
      " 38          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  3  39866368  models.common.C3                        [2048, 2048, 3, False]        \n",
      " 40[27, 30, 33, 36, 39]  1     71514  models.yolo.DetectRAYOLO                [1, [[4, 5, 8, 10, 12, 15], [10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326], [200, 180, 300, 280, 620, 600]], [128, 256, 512, 1024, 2048]]\n",
      "ra_yolo5l summary: 551 layers, 176194010 parameters, 176194010 gradients\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1  18878464  models.common.Conv                      [1024, 2048, 3, 2]            \n",
      " 10                -1  3  39866368  models.common.C3                        [2048, 2048, 3]               \n",
      " 11                -1  1  10491904  models.common.SPPF                      [2048, 2048, 5]               \n",
      " 12                -1  1   2099200  models.common.Conv                      [2048, 1024, 1, 1]            \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3  11020288  models.common.C3                        [2048, 1024, 3, False]        \n",
      " 16                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 25                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 26           [-1, 1]  1         0  models.common.Concat                    [1]                           \n",
      " 27                -1  3    173312  models.common.C3                        [256, 128, 3, False]          \n",
      " 28                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 29          [-1, 24]  1         0  models.common.Concat                    [1]                           \n",
      " 30                -1  3    625152  models.common.C3                        [256, 256, 3, False]          \n",
      " 31                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 32          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 33                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 34                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 35          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 36                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
      " 37                -1  1   9439232  models.common.Conv                      [1024, 1024, 3, 2]            \n",
      " 38          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 39                -1  3  39866368  models.common.C3                        [2048, 2048, 3, False]        \n",
      " 40[27, 30, 33, 36, 39]  1     71514  models.yolo.DetectRAYOLO                [1, [[4, 5, 8, 10, 12, 15], [10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326], [200, 180, 300, 280, 620, 600]], [128, 256, 512, 1024, 2048]]\n",
      "ra_yolo5l summary: 551 layers, 176194010 parameters, 176194010 gradients, 103.5 GFLOPs\n",
      "\n",
      "Transferred 363/917 items from yolov5l.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 151 weight(decay=0.0), 156 weight(decay=0.0005), 156 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mRA-YOLO mode enabled: Disabling fixed size, mosaic, rect, and caching.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/vasil/Desktop/frmdl/datasets/mixed/img/train.cache... 2657 images, 625 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3282/3282 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/vasil/Desktop/frmdl/datasets/mixed/img/val.cache... 680 images, 141 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 821/821 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mERROR: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/runs/train/exp/labels.jpg... \n",
      "/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/train.py:361: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 800 train, 800 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/runs/train/exp\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "  0%|          | 0/3282 [00:00<?, ?it/s]Trainable params: 64920192/176194010 (36.8%)\n",
      "/home/vasil/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/train.py:421: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "        0/0         0G    0.08931    0.04646          0          1       1920:   0%|          | 1/3282 [00:20<18:14:16, 20.01s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06848    0.02536          0          1       1350:   0%|          | 2/3282 [00:28<11:51:04, 13.01s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06928     0.0188          0          2        854:   0%|          | 3/3282 [00:31<7:49:46,  8.60s/it] Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.06358    0.02583          0          5       1920:   0%|          | 4/3282 [00:50<11:33:08, 12.69s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.06524    0.03055          0         18       1920:   0%|          | 5/3282 [01:08<13:15:34, 14.57s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06695    0.02671          0          4        854:   0%|          | 6/3282 [01:11<9:51:22, 10.83s/it] Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.06797    0.02966          0          4       1920:   0%|          | 7/3282 [01:30<12:12:23, 13.42s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06989    0.02654          0          2        854:   0%|          | 8/3282 [01:34<9:25:58, 10.37s/it] Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.07106    0.02407          0          1       1350:   0%|          | 9/3282 [01:42<8:48:58,  9.70s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.06395    0.02631          0          0       1920:   0%|          | 10/3282 [02:01<11:14:15, 12.36s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06548    0.02434          0          3       1350:   0%|          | 11/3282 [02:09<10:00:39, 11.02s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.06588    0.02626          0          4       1920:   0%|          | 12/3282 [02:27<12:05:39, 13.31s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06601     0.0254          0          9        854:   0%|          | 13/3282 [02:31<9:26:38, 10.40s/it] Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G     0.0613     0.0239          0          0       1350:   0%|          | 14/3282 [02:39<8:50:40,  9.74s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05857    0.02261          0          2       1350:   0%|          | 15/3282 [02:47<8:23:12,  9.24s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05994    0.02163          0          4        854:   0%|          | 16/3282 [02:51<6:49:38,  7.53s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06175    0.02061          0          2        854:   1%|          | 17/3282 [02:54<5:41:19,  6.27s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.06259    0.01977          0          5       1350:   1%|          | 18/3282 [03:01<6:00:28,  6.63s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G     0.0593    0.01894          0          0        854:   1%|          | 19/3282 [03:05<5:03:10,  5.57s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.05852    0.02036          0          5       1920:   1%|          | 20/3282 [03:23<8:39:13,  9.55s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.05945    0.02167          0          9       1920:   1%|          | 21/3282 [03:42<11:04:41, 12.23s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05675    0.02087          0          0        854:   1%|          | 22/3282 [03:45<8:38:10,  9.54s/it] Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.05736    0.02208          0         15       1920:   1%|          | 23/3282 [04:02<10:32:00, 11.64s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05773     0.0214          0          3       1350:   1%|          | 24/3282 [04:09<9:29:32, 10.49s/it] Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G    0.05849    0.02247          0         12       1920:   1%|          | 25/3282 [04:26<11:06:20, 12.28s/it]Trainable params: 64920192/176194010 (36.8%)\n",
      "        0/0         0G      0.059    0.02343          0          7       1920:   1%|          | 26/3282 [04:42<12:10:26, 13.46s/it]Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05943    0.02281          0          2        854:   1%|          | 27/3282 [04:46<9:36:30, 10.63s/it] Trainable params: 19271296/176194010 (10.9%)\n",
      "        0/0         0G    0.05943    0.02281          0          2        854:   1%|          | 27/3282 [04:46<9:36:38, 10.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     11\u001b[39m cfg = \u001b[33m\"\u001b[39m\u001b[33mra_yolo5l.yaml\u001b[39m\u001b[33m\"\u001b[39m  \n\u001b[32m     12\u001b[39m opt = argparse.Namespace(\n\u001b[32m     13\u001b[39m     weights=\u001b[33m'\u001b[39m\u001b[33myolov5l.pt\u001b[39m\u001b[33m'\u001b[39m,  \u001b[38;5;66;03m# Model weights\u001b[39;00m\n\u001b[32m     14\u001b[39m     cfg=cfg,  \u001b[38;5;66;03m# Empty to use weights' default config\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCallbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/train.py:698\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(opt, callbacks)\u001b[39m\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt.evolve:\n\u001b[32m--> \u001b[39m\u001b[32m698\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;66;03m# Evolve hyperparameters (optional)\u001b[39;00m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    702\u001b[39m     \u001b[38;5;66;03m# Hyperparameter evolution metadata (including this hyperparameter True-False, lower_limit, upper_limit)\u001b[39;00m\n\u001b[32m    703\u001b[39m     meta = {\n\u001b[32m    704\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlr0\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[32m1e-5\u001b[39m, \u001b[32m1e-1\u001b[39m),  \u001b[38;5;66;03m# initial learning rate (SGD=1E-2, Adam=1E-3)\u001b[39;00m\n\u001b[32m    705\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlrf\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[32m0.01\u001b[39m, \u001b[32m1.0\u001b[39m),  \u001b[38;5;66;03m# final OneCycleLR learning rate (lr0 * lrf)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    732\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcopy_paste\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m),  \u001b[38;5;66;03m# segment copy-paste (probability)\u001b[39;00m\n\u001b[32m    733\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/train.py:422\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(hyp, opt, device, callbacks)\u001b[39m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(amp):\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[32m    423\u001b[39m     loss, loss_items = compute_loss(pred, targets.to(device))  \u001b[38;5;66;03m# loss scaled by batch_size\u001b[39;00m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/models/yolo.py:499\u001b[39m, in \u001b[36mDetectionModelRaYOLO.forward\u001b[39m\u001b[34m(self, x, augment, profile, visualize)\u001b[39m\n\u001b[32m    497\u001b[39m     x = m.forward(x, H)\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m     x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/models/common.py:247\u001b[39m, in \u001b[36mC3.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    246\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Performs forward propagation using concatenated outputs from two convolutions and a Bottleneck sequence.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv3(torch.cat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.cv2(x)), \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/models/common.py:181\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    178\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Processes input through two convolutions, optionally adds shortcut if channel dimensions match; input is a\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[33;03m    tensor.\u001b[39;00m\n\u001b[32m    180\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.add \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28mself\u001b[39m.cv1(x))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/frmdl/frmdl-object-detection-distance-resolution/yolov5/models/common.py:87\u001b[39m, in \u001b[36mConv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     86\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Applies a convolution followed by batch normalization and an activation function to the input tensor `x`.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.bn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/dsait-cv/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "yolov5_dir = os.path.abspath('yolov5')\n",
    "if yolov5_dir not in sys.path:\n",
    "    sys.path.append(yolov5_dir)\n",
    "from yolov5.train import main, Callbacks\n",
    "\n",
    "cfg = \"ra_yolo5l.yaml\"  \n",
    "opt = argparse.Namespace(\n",
    "    weights='yolov5l.pt',  # Model weights\n",
    "    cfg=cfg,  # Empty to use weights' default config\n",
    "    data=os.path.abspath('../datasets/mixed/data.yaml'),  # Absolute path to dataset\n",
    "    hyp=os.path.join(yolov5_dir, 'data/hyps/hyp.scratch-low.yaml'),  # Hyperparameters\n",
    "    epochs=1,\n",
    "    batch_size=1,\n",
    "    imgsz=800,  # Fixed from invalid imgsz=4\n",
    "    rect=False,\n",
    "    resume=False,\n",
    "    nosave=False,\n",
    "    noval=False,\n",
    "    noautoanchor=False,\n",
    "    noplots=False,\n",
    "    evolve=None,\n",
    "    evolve_population=os.path.join(yolov5_dir, 'data/hyps'),\n",
    "    resume_evolve=None,\n",
    "    bucket='',\n",
    "    cache=None,\n",
    "    image_weights=False,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    multi_scale=False,\n",
    "    single_cls=False,\n",
    "    optimizer='SGD',\n",
    "    sync_bn=False,\n",
    "    workers=8,\n",
    "    project=os.path.join(yolov5_dir, 'runs/train'),\n",
    "    name='exp',\n",
    "    exist_ok=True,\n",
    "    quad=False,\n",
    "    cos_lr=False,\n",
    "    label_smoothing=0.0,\n",
    "    patience=100,\n",
    "    freeze=[0],  # Freeze first 10 layers\n",
    "    save_period=-1,\n",
    "    seed=0,\n",
    "    local_rank=-1,\n",
    "    ra_yolo=True,\n",
    "    entity=None,\n",
    "    upload_dataset=False,\n",
    "    bbox_interval=-1,\n",
    "    artifact_alias='latest',\n",
    "    ndjson_console=False,\n",
    "    ndjson_file=False\n",
    "\n",
    ")\n",
    "\n",
    "main(opt, callbacks=Callbacks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b06c1c",
   "metadata": {},
   "source": [
    "#### Extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b31f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from yolov5.val import run  as yolov5_run_val\n",
    "\n",
    "data = '../datasets/mixed/data.yaml'  # Path to your dataset YAML file\n",
    "weights = 'yolov5n.pt'      # Path to your model weights file\n",
    "batch_size = 16             # Batch size for validation\n",
    "imgsz = 4                # Image size for inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Automatically select device\n",
    "\n",
    "# Run the validation\n",
    "results, maps, times = yolov5_run_val(\n",
    "    data=data,              # Dataset configuration\n",
    "    weights=weights,        # Model weights\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    imgsz=imgsz,            # Image size\n",
    "    device=device,          # Device to run on\n",
    "    task='val',             # Task type: validation\n",
    "    save_txt=True,         # Don‚Äôt save results to text files\n",
    "    save_json=True,        # Don‚Äôt save results to JSON\n",
    "    plots=True,             # Generate plots (saved to runs/val/)\n",
    ")\n",
    "\n",
    "# Extract metrics from r\n",
    "mp, mr, map50, map, box_loss, obj_loss, cls_loss = results  # Mean Precision, Mean Recall, mAP@0.5, mAP@0.5:0.95\n",
    "print(f'Mean Precision: {mp:.4f}')\n",
    "print(f'Mean Recall: {mr:.4f}')\n",
    "print(f'mAP@0.5: {map50:.4f}')\n",
    "print(f'mAP@0.5:0.95: {map:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb822c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, im_names, label_names = extract_predictions_from_run(json_path=\"runs/val/exp/predictions.json\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8894b32",
   "metadata": {},
   "source": [
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb5840",
   "metadata": {},
   "source": [
    "#### Reproducing figures from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc6f29",
   "metadata": {},
   "source": [
    "#### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.xy_lineplot import basic_lineplot, multi_lineplot\n",
    "\n",
    "data_dict = {\n",
    "    \"EuroCity Original -> Finetuned Model\": ([0, 1, 2, 3, 4, 5], [0, 1, 4, 9, 16, 25]),\n",
    "    \"EuroCity 1.42 -> Finetuned Model\": ([0, 1, 2, 6, 8, 9], [0, 1, 5, 10, 16, 27]),\n",
    "    \"EuroCity Original\": ([0, 2, 3, 5, 7, 9], [0, 1, 2, 3, 12, 20]), \n",
    "    \"EuroCity 1.42\": ([0, 2, 2, 4, 8, 9], [0, 1, 4, 9, 16, 25])\n",
    "}\n",
    "## Figure 5 in the paper \n",
    "multi_lineplot(data_dict, xlabel=\"Megabytes/Image\", ylabel=\"map@50 - All Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.xy_lineplot import basic_lineplot, multi_lineplot\n",
    "\n",
    "data_dict = {\n",
    "    \"EuroCity Original\": ([0, 1, 2, 3, 4, 5], [25, 16, 10, 7, 3, 1]),\n",
    "    \"EuroCity 1.42 -> Finetuned Model\": ([0, 1, 2, 6, 8, 9], [24, 13, 10, 5, 1, 0]),\n",
    "}\n",
    "## Figure 5 in the paper \n",
    "multi_lineplot(data_dict, xlabel=\"Distance(m)\", ylabel=\"Recall-Pedestrian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsait-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
