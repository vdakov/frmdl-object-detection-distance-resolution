{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ba7105",
   "metadata": {},
   "source": [
    "# Understanding the Impact of Image Quality and Distance of Objects to Object Detection Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458bd6c9",
   "metadata": {},
   "source": [
    "***A reproduction*** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c63682",
   "metadata": {},
   "source": [
    "## Downscale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d24d59",
   "metadata": {},
   "source": [
    "Folder expansion: Some datasets come with multiple subfolders not needed for the purposes of our reproduction study. Example:\n",
    "\n",
    "```\n",
    "img\n",
    "|_dir1 \n",
    "    |_img1.png\n",
    "    |_img2.png\n",
    "    |_img3.png\n",
    "|_dir2\n",
    "    |_img1.png\n",
    "    |_img2.png\n",
    "    |_img3.png\n",
    "|_dir3\n",
    "(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_folders import expand_folders\n",
    "expand_folders(\"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e06b67",
   "metadata": {},
   "source": [
    "### Spatial and Amplitudinal Resolution Downsampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302f025",
   "metadata": {},
   "source": [
    "Spatial - 1.42x Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058db19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/spatially_compressed\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    scale_factors=[720.0/1024], #value from paper \n",
    "    qp_values=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326eaffb",
   "metadata": {},
   "source": [
    "Amplitudinal Downsampling"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-13T12:09:51.836623Z",
     "start_time": "2025-06-13T12:08:04.167257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/eurocity_original_amplitudinally_compressed\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "COMPRESSION_METADATA_DIR = f\"{DATASET_OUTPUT_DIR}/metadata\"\n",
    "\n",
    "qp_values = [16, 24, 34, 38, 46] #values from paper\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    scale_factors=[],\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    metadata_dir=COMPRESSION_METADATA_DIR,\n",
    "    qp_values=qp_values, #values from paper\n",
    "    expansion = \"amplitudinal\"\n",
    ")"
   ],
   "id": "c67ba863d04e8c6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Processing amplitudinal downsampling with 23 parallel processes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing amplitudinal downsampling: 100%|██████████| 860/860 [01:46<00:00,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images expanded and saved to D:\\code-projects\\Python_projects\\datasets\\eurocity_original_amplitudinally_compressed\\img and D:\\code-projects\\Python_projects\\datasets\\eurocity_original_amplitudinally_compressed\\labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b61c61eb",
   "metadata": {},
   "source": [
    "Mixed Downsampling - Combined Set of both"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from data_processing.expand_dataset import expand_dataset\n",
    "\n",
    "INPUT_IMAGE_DIR = \"../datasets/ECP/day/img/val\"\n",
    "INPUT_LABEL_DIR = \"../datasets/ECP2dot5D_day_labels_val/ECP2dot5D/day/labels/val\"\n",
    "DATASET_OUTPUT_DIR = \"../datasets/test\"\n",
    "label_values_to_scale = [\"imageheight\", \"imagewidth\", \"x0\", \"y0\", \"x1\", \"y1\"]\n",
    "OUTPUT_IMG_DIR = f\"{DATASET_OUTPUT_DIR}/img\"\n",
    "OUTPUT_LABEL_DIR = f\"{DATASET_OUTPUT_DIR}/labels\"\n",
    "COMPRESSION_METADATA_DIR = f\"{DATASET_OUTPUT_DIR}/metadata\"\n",
    "qp_values = [16, 24, 34, 38, 46] #values from paper\n",
    "\n",
    "expand_dataset(\n",
    "    input_dir=INPUT_IMAGE_DIR,\n",
    "    label_dir=INPUT_LABEL_DIR,\n",
    "    label_values_to_scale=label_values_to_scale,\n",
    "    output_img_dir=OUTPUT_IMG_DIR,\n",
    "    output_label_dir=OUTPUT_LABEL_DIR,\n",
    "    metadata_dir=COMPRESSION_METADATA_DIR,\n",
    "    scale_factors=[1, 720.0/1024, 854.0/ 1920], #values from paper\n",
    "    qp_values=[0, 5, 10, 16, 20, 24, 28, 34, 38, 42, 46, 51], #values from paper\n",
    "    expansion=\"mixed\", \n",
    "    subsample_spatial=True,\n",
    "    subsample_amplitudinal=True,\n",
    ")"
   ],
   "id": "114e8eb846b4659e"
  },
  {
   "cell_type": "markdown",
   "id": "703a057a",
   "metadata": {},
   "source": [
    "### Convert to YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8663a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.to_yolo_format import to_yolo_format\n",
    "\n",
    "labels_dir = \"../datasets/mixed/labels\"     # YOLO output\n",
    "images_dir = \"../datasets/mixed/img\"     # Images directory\n",
    "dataset_root = \"../datasets/mixed\"          # Root directory\n",
    "split = 0.8\n",
    "\n",
    "\n",
    "to_yolo_format(\n",
    "    labels_dir=labels_dir,\n",
    "    images_dir=images_dir,\n",
    "    dataset_root=dataset_root,\n",
    "    split=split,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57854c",
   "metadata": {},
   "source": [
    "## Train/Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2259e3",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ee050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "yolov5_dir = os.path.abspath('yolov5')\n",
    "if yolov5_dir not in sys.path:\n",
    "    sys.path.append(yolov5_dir)\n",
    "from yolov5.train import main, Callbacks\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cfg = \"ra_yolo5l.yaml\"  \n",
    "opt = argparse.Namespace(\n",
    "    weights='yolov5l.pt',  # Model weights\n",
    "    cfg=cfg,  # Empty to use weights' default config\n",
    "    data=os.path.abspath('../datasets/mixed/data.yaml'),  # Absolute path to dataset\n",
    "    hyp=os.path.join(yolov5_dir, 'data/hyps/hyp.scratch-low.yaml'),  # Hyperparameters\n",
    "    epochs=1,\n",
    "    batch_size=1,\n",
    "    imgsz=800,  # Fixed from invalid imgsz=4\n",
    "    rect=False,\n",
    "    resume=False,\n",
    "    nosave=False,\n",
    "    noval=False,\n",
    "    noautoanchor=False,\n",
    "    noplots=False,\n",
    "    evolve=None,\n",
    "    evolve_population=os.path.join(yolov5_dir, 'data/hyps'),\n",
    "    resume_evolve=None,\n",
    "    bucket='',\n",
    "    cache=None,\n",
    "    image_weights=False,\n",
    "    device=device,\n",
    "    multi_scale=False,\n",
    "    single_cls=False,\n",
    "    optimizer='SGD',\n",
    "    sync_bn=False,\n",
    "    workers=8,\n",
    "    project=os.path.join(yolov5_dir, 'runs/train'),\n",
    "    name='exp',\n",
    "    exist_ok=True,\n",
    "    quad=False,\n",
    "    cos_lr=False,\n",
    "    label_smoothing=0.0,\n",
    "    patience=100,\n",
    "    freeze=[0],  # Freeze first 10 layers\n",
    "    save_period=-1,\n",
    "    seed=0,\n",
    "    local_rank=-1,\n",
    "    ra_yolo=True,\n",
    "    entity=None,\n",
    "    upload_dataset=False,\n",
    "    bbox_interval=-1,\n",
    "    artifact_alias='latest',\n",
    "    ndjson_console=False,\n",
    "    ndjson_file=False\n",
    ")\n",
    "\n",
    "main(opt, callbacks=Callbacks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b06c1c",
   "metadata": {},
   "source": [
    "#### Extract results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b31f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from yolov5.val import run  as yolov5_run_val\n",
    "\n",
    "data = '../datasets/mixed/data.yaml'  # Path to your dataset YAML file\n",
    "weights = 'yolov5/runs/train/exp/weights/best.pt'      # Path to your model weights file\n",
    "batch_size = 1             # Batch size for validation           # Image size for inference\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Automatically select device\n",
    "confidence_threshold = 0.1\n",
    "\n",
    "\n",
    "# Run the validation\n",
    "results, maps, times = yolov5_run_val(\n",
    "    data=data,              # Dataset configuration\n",
    "    imgsz=4,\n",
    "    weights=weights,        # Model weights\n",
    "    batch_size=batch_size,  # Batch size         # Image size\n",
    "    device=device,          # Device to run on\n",
    "    task='val',             # Task type: validation\n",
    "    save_txt=True,         # Don’t save results to text files\n",
    "    save_json=True,        # Don’t save results to JSON\n",
    "    plots=True,             # Generate plots (saved to runs/val/)\n",
    "    confidence=confidence_threshold,  # Confidence threshold for predictions\n",
    ")\n",
    "\n",
    "# Extract metrics from r\n",
    "mp, mr, map50, map, box_loss, obj_loss, cls_loss = results  # Mean Precision, Mean Recall, mAP@0.5, mAP@0.5:0.95\n",
    "print(f'Mean Precision: {mp:.4f}')\n",
    "print(f'Mean Recall: {mr:.4f}')\n",
    "print(f'mAP@0.5: {map50:.4f}')\n",
    "print(f'mAP@0.5:0.95: {map:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb822c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_predictions_from_run(json_path, label_dir=None):\n",
    "    import json\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    predictions = {}\n",
    "    labels = {}\n",
    "    \n",
    "    \n",
    "    for item in data:\n",
    "        if item['image_id'] not in predictions.keys():\n",
    "            predictions[item['image_id']] = []\n",
    "        if item['image_id'] not in labels.keys():\n",
    "            labels[item['image_id']] = json.load(open(f\"{label_dir}/{item['image_id']}.json\")) if label_dir else {}\n",
    "            \n",
    "        predictions[item['image_id']].append(item)\n",
    "    \n",
    "    return predictions, labels\n",
    "\n",
    "predictions, labels = extract_predictions_from_run(json_path=\"yolov5/runs/val/exp13/best_predictions.json\", label_dir=\"../datasets/mixed/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    # Existing IoU calculation remains unchanged\n",
    "    x0_pred, y0_pred, x1_pred, y1_pred = bbox1\n",
    "    x0_gt, y0_gt, x1_gt, y1_gt = bbox2\n",
    "    \n",
    "    x0_inter = max(x0_pred, x0_gt)\n",
    "    y0_inter = max(y0_pred, y0_gt)\n",
    "    x1_inter = min(x1_pred, x1_gt)\n",
    "    y1_inter = min(y1_pred, y1_gt)\n",
    "    \n",
    "    if x0_inter < x1_inter and y0_inter < y1_inter:\n",
    "        intersection_area = (x1_inter - x0_inter) * (y1_inter - y0_inter)\n",
    "    else:\n",
    "        intersection_area = 0\n",
    "    \n",
    "    area_pred = (x1_pred - x0_pred) * (y1_pred - y0_pred)\n",
    "    area_gt = (x1_gt - x0_gt) * (y1_gt - y0_gt)\n",
    "    union_area = area_pred + area_gt - intersection_area\n",
    "    \n",
    "    return intersection_area / union_area if union_area != 0 else 0.0\n",
    "\n",
    "def calculate_distance_to_gt(label):\n",
    "    xyz = label[4]\n",
    "    x, y, z = xyz[\"x\"], xyz[\"y\"], xyz[\"z\"]\n",
    "\n",
    "    return (x**2 + y**2 + z**2)**0.5 if x is not None and y is not None and z is not None else 0.0\n",
    "\n",
    "def create_predictions_dataframe(predictions, labels):\n",
    "    rows = []  # Collect all rows here for batch DataFrame creation\n",
    "    \n",
    "    for im_name in tqdm(predictions.keys()):\n",
    "        # Extract resolutions from filename\n",
    "        parts = im_name.split('_')\n",
    "        spatial_res = float(parts[5][:2])\n",
    "        amp_val = parts[6][2:].split('.')[0]  # Handle file extensions\n",
    "        amplitudal_res = int(amp_val) if amp_val != '' else 0\n",
    "        \n",
    "        # Get current predictions and labels\n",
    "        curr_preds = predictions[im_name]\n",
    "        curr_labels = [obj for obj in labels[im_name][\"children\"] \n",
    "                      if obj['identity'] == \"pedestrian\"]\n",
    "        \n",
    "        gt_bboxes = [\n",
    "            [obj['x0'], obj['y0'], obj['x1'], obj['y1'], obj['3dp'] if '3dp' in obj else {\"x\": None, \"y\": None, \"z\": None}]\n",
    "            for obj in curr_labels\n",
    "        ]\n",
    "        pred_bboxes = [pred['bbox'] for pred in curr_preds]\n",
    "        \n",
    "        matched_pred_indices = set()  # Track indices of matched predictions\n",
    "        \n",
    "        # Process ground truths\n",
    "        for gt_bbox in gt_bboxes:\n",
    "            matched = False\n",
    "            for i, pred_bbox in enumerate(pred_bboxes):\n",
    "                if i in matched_pred_indices:\n",
    "                    continue  # Skip already matched predictions\n",
    "                if calculate_iou(pred_bbox, gt_bbox[:4]) > 0.5:\n",
    "                    matched_pred_indices.add(i)\n",
    "                    matched = True\n",
    "                    break  # Stop after first match\n",
    "            \n",
    "            # Add row for ground truth\n",
    "            rows.append({\n",
    "                'image_id': im_name,\n",
    "                'distance': calculate_distance_to_gt(gt_bbox),\n",
    "                'spatial_res': spatial_res,\n",
    "                'amplitudal_res': amplitudal_res,\n",
    "                'distance_to_gt': 0,\n",
    "                'tp': int(matched),\n",
    "                'fp': 0,\n",
    "                'fn': int(not matched),\n",
    "                'label': 'gt'\n",
    "            })\n",
    "        \n",
    "        # Process unmatched predictions (false positives)\n",
    "        for i, _ in enumerate(pred_bboxes):\n",
    "            if i not in matched_pred_indices:\n",
    "                rows.append({\n",
    "                    'image_id': im_name,\n",
    "                    'distance': 0,\n",
    "                    'spatial_res': spatial_res,\n",
    "                    'amplitudal_res': amplitudal_res,\n",
    "                    'distance_to_gt': 0,\n",
    "                    'tp': 0,\n",
    "                    'fp': 1,\n",
    "                    'fn': 0,\n",
    "                    'label': 'pred'\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "    \n",
    "create_predictions_dataframe(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8894b32",
   "metadata": {},
   "source": [
    "## Visualizing Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb5840",
   "metadata": {},
   "source": [
    "#### Reproducing figures from the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc6f29",
   "metadata": {},
   "source": [
    "#### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46314223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.xy_lineplot import basic_lineplot, multi_lineplot\n",
    "\n",
    "data_dict = {\n",
    "    \"EuroCity Original -> Finetuned Model\": ([0, 1, 2, 3, 4, 5], [0, 1, 4, 9, 16, 25]),\n",
    "    \"EuroCity 1.42 -> Finetuned Model\": ([0, 1, 2, 6, 8, 9], [0, 1, 5, 10, 16, 27]),\n",
    "    \"EuroCity Original\": ([0, 2, 3, 5, 7, 9], [0, 1, 2, 3, 12, 20]), \n",
    "    \"EuroCity 1.42\": ([0, 2, 2, 4, 8, 9], [0, 1, 4, 9, 16, 25])\n",
    "}\n",
    "## Figure 5 in the paper \n",
    "multi_lineplot(data_dict, xlabel=\"Megabytes/Image\", ylabel=\"map@50 - All Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb13fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.xy_lineplot import basic_lineplot, multi_lineplot\n",
    "\n",
    "data_dict = {\n",
    "    \"EuroCity Original\": ([0, 1, 2, 3, 4, 5], [25, 16, 10, 7, 3, 1]),\n",
    "    \"EuroCity 1.42 -> Finetuned Model\": ([0, 1, 2, 6, 8, 9], [24, 13, 10, 5, 1, 0]),\n",
    "}\n",
    "## Figure 5 in the paper \n",
    "multi_lineplot(data_dict, xlabel=\"Distance(m)\", ylabel=\"Recall-Pedestrian\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
